{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Engineer Nanodegree\n",
    "## Capstone Project\n",
    "Armen Donigian  \n",
    "December 6th, 2016\n",
    "\n",
    "## I. Definition\n",
    "\n",
    "### Project Overview\n",
    "\n",
    "This project is in response to a Kaggle competition to build a predictive model to determine the sale price of a property within the Ames, Iowa region. See [**House Prices: Advanced Regression Techniques**](https://www.kaggle.com/c/house-prices-advanced-regression-techniques) for further details. \n",
    "\n",
    "Besides being a home, real estate has been a very popular investment. While most of us realize the number of bedrooms or bathrooms would influence the price of a home, it turns out that other features such as the size of the garage or proximity to a particular landmark may influence the price just as much. \n",
    "\n",
    "The gain from an investment (such as a house in our case) is significantly impacted by the purchase price, which is why the answer for this competition matters.\n",
    "\n",
    "The competition began on August 30, 2016 while the final submission must be made by March 1st, 2017. \n",
    "\n",
    "The source code for this project is available via a Jupyter notebook located [here](./Capstone-Project-Ames-Housing-Data.ipynb).\n",
    "\n",
    "### Problem Statement\n",
    "With 79 explanatory variables describing (almost) every aspect of residential homes in **Ames, Iowa**, this competition challenges you to predict the final price of each home.\n",
    "\n",
    "Here's an end-to-end workflow for building a regressor using scikit-learn.\n",
    "\n",
    "1. Problem Definition (Ames house price data). \n",
    "\t+ Experimental Design: identify data sources, formats, data dictionary, features and target. \n",
    "\n",
    "2. Loading the Dataset\n",
    "\t+ Load and pre-process the data into a representation which is ready for model training\n",
    "\n",
    "3. Exploratory Data Analysis \n",
    "\t+ Gather insights by using exploratory methods (univariate feature distributions, skew and correlated attributes to name a few)\n",
    "\n",
    "4. Feature Engineering  \n",
    "\n",
    "  + calculate age of property (relative to oldest)\n",
    "  + years till remodel\n",
    "  + linear combinations of square footage attributes (DID NOT HELP!)\n",
    "  + one hot encoding of categorical features\n",
    "  \n",
    "5. Feature Selection\n",
    "  + dropping highly correlated features (see correlation section above)\n",
    "  + PCA\n",
    "  + Recusive Feature Elimination (DID NOT HELP!)\n",
    "  + LASSO\n",
    "\n",
    "6. Evaluate Algorithms\n",
    "\n",
    "7. Evaluate Algorithms with Standardization\n",
    "\n",
    "8. Algorithm Tuning\n",
    "\t+ Use GridSearch to search & tune hyper-parameters\n",
    "\n",
    "9. Ensemble Methods (such as Bagging and Boosting, Gradient Boosting looked good).  \n",
    "\n",
    "10. Finalize Model (use all training data and confirm using validation dataset).\n",
    "\n",
    "### Metrics\n",
    "Given this is a Kaggle competition, the evaluation metric has been specified as the [**Root-Mean-Squared-Error (RMSE)**](https://www.kaggle.com/c/outbrain-click-prediction/details/evaluation). \n",
    "\n",
    "![Eval Equation](../assets/ames/Evaluation_Ames__Kaggle.png)\n",
    "\n",
    "The root mean squared error metric intutively is suitable for house price prediction since:\n",
    "\n",
    "+ a regressor outputs numerical predictions which won't exactly match the target values in the data set (due to irreducible error)\n",
    "+ can use the outputs of the regressor to measure difference between values in training dataset vs predicted values by my model\n",
    "\n",
    "\n",
    "## II. Analysis\n",
    "\n",
    "### Data Exploration\n",
    "\n",
    "The following files have been provided for this competition:\n",
    "\n",
    "| File        | Dimensions (rows, columns)          |  Text or Numeric      |  \n",
    "| ------------- |:-------------:| -----:|\n",
    "| train     | \t1459, 81 | Alpha Numeric |\n",
    "| test  (public hold out)    | \t\t1459, 80 | Alpha Numeric |  \n",
    "\n",
    "There are 79 explanatory variables describing (almost) every aspect of residential homes in Ames.\n",
    "\n",
    "```\n",
    "SalePrice - the property's sale price in dollars. This is the target variable that you're trying to predict.\n",
    "MSSubClass: The building class\n",
    "MSZoning: The general zoning classification\n",
    "LotFrontage: Linear feet of street connected to property\n",
    "LotArea: Lot size in square feet\n",
    "Street: Type of road access\n",
    "Alley: Type of alley access\n",
    "LotShape: General shape of property\n",
    "LandContour: Flatness of the property\n",
    "Utilities: Type of utilities available\n",
    "LotConfig: Lot configuration\n",
    "LandSlope: Slope of property\n",
    "Neighborhood: Physical locations within Ames city limits\n",
    "Condition1: Proximity to main road or railroad\n",
    "Condition2: Proximity to main road or railroad (if a second is present)\n",
    "BldgType: Type of dwelling\n",
    "HouseStyle: Style of dwelling\n",
    "OverallQual: Overall material and finish quality\n",
    "OverallCond: Overall condition rating\n",
    "YearBuilt: Original construction date\n",
    "YearRemodAdd: Remodel date\n",
    "RoofStyle: Type of roof\n",
    "RoofMatl: Roof material\n",
    "Exterior1st: Exterior covering on house\n",
    "Exterior2nd: Exterior covering on house (if more than one material)\n",
    "MasVnrType: Masonry veneer type\n",
    "MasVnrArea: Masonry veneer area in square feet\n",
    "ExterQual: Exterior material quality\n",
    "ExterCond: Present condition of the material on the exterior\n",
    "Foundation: Type of foundation\n",
    "BsmtQual: Height of the basement\n",
    "BsmtCond: General condition of the basement\n",
    "BsmtExposure: Walkout or garden level basement walls\n",
    "BsmtFinType1: Quality of basement finished area\n",
    "BsmtFinSF1: Type 1 finished square feet\n",
    "BsmtFinType2: Quality of second finished area (if present)\n",
    "BsmtFinSF2: Type 2 finished square feet\n",
    "BsmtUnfSF: Unfinished square feet of basement area\n",
    "TotalBsmtSF: Total square feet of basement area\n",
    "Heating: Type of heating\n",
    "HeatingQC: Heating quality and condition\n",
    "CentralAir: Central air conditioning\n",
    "Electrical: Electrical system\n",
    "1stFlrSF: First Floor square feet\n",
    "2ndFlrSF: Second floor square feet\n",
    "LowQualFinSF: Low quality finished square feet (all floors)\n",
    "GrLivArea: Above grade (ground) living area square feet\n",
    "BsmtFullBath: Basement full bathrooms\n",
    "BsmtHalfBath: Basement half bathrooms\n",
    "FullBath: Full bathrooms above grade\n",
    "HalfBath: Half baths above grade\n",
    "Bedroom: Number of bedrooms above basement level\n",
    "Kitchen: Number of kitchens\n",
    "KitchenQual: Kitchen quality\n",
    "TotRmsAbvGrd: Total rooms above grade (does not include bathrooms)\n",
    "Functional: Home functionality rating\n",
    "Fireplaces: Number of fireplaces\n",
    "FireplaceQu: Fireplace quality\n",
    "GarageType: Garage location\n",
    "GarageYrBlt: Year garage was built\n",
    "GarageFinish: Interior finish of the garage\n",
    "GarageCars: Size of garage in car capacity\n",
    "GarageArea: Size of garage in square feet\n",
    "GarageQual: Garage quality\n",
    "GarageCond: Garage condition\n",
    "PavedDrive: Paved driveway\n",
    "WoodDeckSF: Wood deck area in square feet\n",
    "OpenPorchSF: Open porch area in square feet\n",
    "EnclosedPorch: Enclosed porch area in square feet\n",
    "3SsnPorch: Three season porch area in square feet\n",
    "ScreenPorch: Screen porch area in square feet\n",
    "PoolArea: Pool area in square feet\n",
    "PoolQC: Pool quality\n",
    "Fence: Fence quality\n",
    "MiscFeature: Miscellaneous feature not covered in other categories\n",
    "MiscVal: $Value of miscellaneous feature\n",
    "MoSold: Month Sold\n",
    "YrSold: Year Sold\n",
    "SaleType: Type of sale\n",
    "SaleCondition: Condition of sale\n",
    "```\n",
    "\n",
    "Kaggle maintains a portion of the data for each competition as a hold out set which is used to evaluate the generalizability of our model to new unseen data (aka private leaderboard). The Public Score is being determined from only a fraction of the test data set (25-33%).\n",
    "\n",
    "\n",
    "Sample data:\n",
    "\n",
    "```\n",
    "Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape LandContour Utilities  \\\n",
    "0        1          60       RL         65.0     8450   Pave   NaN      Reg         Lvl    AllPub   \n",
    "1        2          20       RL         80.0     9600   Pave   NaN      Reg         Lvl    AllPub   \n",
    "2        3          60       RL         68.0    11250   Pave   NaN      IR1         Lvl    AllPub   \n",
    "3        4          70       RL         60.0     9550   Pave   NaN      IR1         Lvl    AllPub   \n",
    "4        5          60       RL         84.0    14260   Pave   NaN      IR1         Lvl    AllPub   \n",
    "5        6          50       RL         85.0    14115   Pave   NaN      IR1         Lvl    AllPub   \n",
    "6        7          20       RL         75.0    10084   Pave   NaN      Reg         Lvl    AllPub   \n",
    "7        8          60       RL          NaN    10382   Pave   NaN      IR1         Lvl    AllPub\n",
    "\n",
    " PoolArea PoolQC  Fence MiscFeature MiscVal MoSold YrSold  SaleType  SaleCondition  \\\n",
    "0       ...            0    NaN    NaN         NaN       0      2   2008        WD         Normal   \n",
    "1       ...            0    NaN    NaN         NaN       0      5   2007        WD         Normal   \n",
    "2       ...            0    NaN    NaN         NaN       0      9   2008        WD         Normal   \n",
    "3       ...            0    NaN    NaN         NaN       0      2   2006        WD        Abnorml   \n",
    "4       ...            0    NaN    NaN         NaN       0     12   2008        WD         Normal   \n",
    "5       ...            0    NaN  MnPrv        Shed     700     10   2009        WD         Normal   \n",
    "6       ...            0    NaN    NaN         NaN       0      8   2007        WD         Normal   \n",
    "7       ...            0    NaN    NaN        Shed     350     11   2009        WD         Normal   \\\n",
    "\n",
    "\n",
    "      SalePrice  \n",
    "0        12.248  \n",
    "1        12.109  \n",
    "2        12.317  \n",
    "3        11.849  \n",
    "4        12.429  \n",
    "5        11.871  \n",
    "6        12.635  \n",
    "7        12.206  \n",
    "8        11.775\n",
    "```\n",
    "\n",
    "Here's a summary of the missing values in the train dataset file provided:\n",
    "\n",
    "| File        | # of missing          |  \n",
    "| ------------- |:-------------:|  \n",
    "| LotFrontage     | 259\t | \n",
    "| Alley    | \t1368\t  |  \n",
    "| MasVnrType\t| 8\t| \n",
    "| MasVnrArea\t| 8\t| \n",
    "| BsmtQual\t|\t37| \n",
    "| BsmtCond\t|\t37| \n",
    "| BsmtExposure\t|\t38| \n",
    "| BsmtFinType1\t|\t37| \n",
    "| BsmtFinType2\t| 38\t| \n",
    "| Electrical\t|\t1| \n",
    "| FireplaceQu\t|\t689| \n",
    "| GarageType\t|\t81| \n",
    "| GarageYrBlt\t|\t81| \n",
    "| GarageFinish |\t81| \n",
    "| GarageQual\t|\t81| \n",
    "| GarageCond |\t81| \n",
    "| PoolQC\t|1452\t| \n",
    "| Fence\t|1178\t| \n",
    "| MiscFeature\t| 1405\t| \n",
    " \n",
    "#### Descriptive Statistics\n",
    "It's important to note that Id column should not be used as a feature in our training set and will hence be removed.\n",
    "\n",
    "The missing values (NAN) impact the results of our descriptive stats. I'll be using the imputer module from scikit-learn to impute mean values for those missing.\n",
    "\n",
    "1459 rows of training data (test dataset same dimensions) may not be enough to build a predictive model due to it's small size, but let's see what we can do with what we have since gathering new data takes more time, money or might just not be possible. \n",
    "\n",
    "Since MSSubClass is a type identifier for type of dwelling, it wouldn't make sense to look at the summary statistics. \n",
    "\n",
    "OverallQual with a mean of 6.0, OverallCond with mean of 5.5 seems reasonable and nice to see the data confirm what we would intuitively expect. \n",
    "\n",
    "Oldest property built in 1879 while most recent in 2010. Since price of a property is determined not only by where it's located but also when it's purchased. This is something we want to keep in mind since we know the real estate market is susceptible to bubbles followed by crashes.\n",
    "\n",
    "Which brings us to our target variable SalePrice. Cheapest property sold for 39K, while most expensive 755K with a mean of 180,944. I will create candle stick plots to do outlier analysis.\n",
    "\n",
    "```\n",
    "Id  MSSubClass  LotFrontage     LotArea  OverallQual  OverallCond  YearBuilt  \\\n",
    "count  1459.000    1459.000     1200.000    1459.000     1459.000     1459.000   1459.000   \n",
    "mean    730.000      56.923       70.046   10517.225        6.100        5.575   1971.272   \n",
    "std     421.321      42.304       24.294    9984.676        1.383        1.113     30.213   \n",
    "min       1.000      20.000       21.000    1300.000        1.000        1.000   1872.000   \n",
    "25%     365.500      20.000          NaN    7549.000        5.000        5.000   1954.000   \n",
    "50%     730.000      50.000          NaN    9477.000        6.000        5.000   1973.000   \n",
    "75%    1094.500      70.000          NaN   11603.000        7.000        6.000   2000.000   \n",
    "max    1459.000     190.000      313.000  215245.000       10.000        9.000   2010.000\n",
    "\n",
    "       YearRemodAdd  MasVnrArea  BsmtFinSF1     ...      WoodDeckSF  OpenPorchSF  EnclosedPorch  \\\n",
    "count      1459.000    1451.000    1459.000     ...        1459.000     1459.000       1459.000   \n",
    "mean       1984.879     103.757     443.375     ...          93.805       46.646         21.969   \n",
    "std          20.646     181.108     456.142     ...         124.249       66.276         61.137   \n",
    "min        1950.000       0.000       0.000     ...           0.000        0.000          0.000   \n",
    "25%        1967.000         NaN       0.000     ...           0.000        0.000          0.000   \n",
    "50%        1994.000         NaN     383.000     ...           0.000       25.000          0.000   \n",
    "75%        2004.000         NaN     712.000     ...         168.000       68.000          0.000   \n",
    "max        2010.000    1600.000    5644.000     ...         857.000      547.000        552.000\n",
    "\n",
    "       3SsnPorch  ScreenPorch  PoolArea    MiscVal    MoSold    YrSold   SalePrice  \n",
    "count   1459.000     1459.000  1459.000   1459.000  1459.000  1459.000    1459.000  \n",
    "mean       3.412       15.071     2.761     43.519     6.322  2007.816  180944.103  \n",
    "std       29.327       55.775    40.191    496.292     2.705     1.329   79464.918  \n",
    "min        0.000        0.000     0.000      0.000     1.000  2006.000   34900.000  \n",
    "25%        0.000        0.000     0.000      0.000     5.000  2007.000  129950.000  \n",
    "50%        0.000        0.000     0.000      0.000     6.000  2008.000  163000.000  \n",
    "75%        0.000        0.000     0.000      0.000     8.000  2009.000  214000.000  \n",
    "max      508.000      480.000   738.000  15500.000    12.000  2010.000  755000.000 \n",
    "```\n",
    "\n",
    "\n",
    "### Exploratory Visualization\n",
    "\n",
    "#### Correlations Between Attributes\n",
    "\n",
    "Correlation refers to the relationship between two variables and how they may or may not change together. The most common method for calculating correlation is Pearsonâ€™s Correlation Coefficient (assumes normal distribution). A correlation of -1 or 1 shows a full negative or positive correlation respectively, while a value of 0 shows no correlation at all. Some machine learning algorithms like linear and logistic regression can suffer poor performance if there are highly correlated attributes in your dataset. Let's review all of the pairwise correlations of the attributes in your dataset. \n",
    "\n",
    "![Eval Equation](../assets/ames/correlation_plot.png)\n",
    "\n",
    "Here are the top feature correlations:\n",
    "\n",
    "+ YearBuilt & GarageYrBlt\n",
    "+ BsmtFullBath & BsmtFinSF1\n",
    "+ TotalBsmtSF & 1stFlrSF\n",
    "+ TotRmsAbvGrd & GrLivArea\n",
    "+ GarageCars & GarageArea\n",
    "\n",
    "There are some obvious correlations with the SalePrice (target):\n",
    "\n",
    "+ Rates the overall material and finish of the house \n",
    "+ Basement square footage\n",
    "+ Year built and remodeled\n",
    "+ 1st Floor square footage\n",
    "+ Living area square footage\n",
    "+ Number of full bathrooms\n",
    "+ Total rooms above ground\n",
    "+ Size of garage in car capacity\n",
    "+ Size of garage in square feet\n",
    "\n",
    "Some not so obvious features which didn't show strong correlation with target:\n",
    "\n",
    "+ Year sold (housing bubbles)\n",
    "+ Month sold (summer $$$)\n",
    "+ Kitchen quality\n",
    "+ Overall condition of the house\n",
    "+ Type of dwelling involved in the sale\n",
    "\n",
    "List the numerical features decendingly by their correlation with Sale Price:\n",
    "\n",
    "```\n",
    "OverallQual: \t0.790971646739\n",
    "GrLivArea: \t0.708584256389\n",
    "GarageCars: \t0.640383308726\n",
    "GarageArea: \t0.623384913038\n",
    "TotalBsmtSF: \t0.613791532285\n",
    "1stFlrSF: \t0.605970779941\n",
    "FullBath: \t0.560604113108\n",
    "TotRmsAbvGrd: \t0.533682199367\n",
    "YearBuilt: \t0.522876912022\n",
    "YearRemodAdd: \t0.507015099034\n",
    "GarageYrBlt: \t0.486264369585\n",
    "MasVnrArea: \t0.477410802037\n",
    "Fireplaces: \t0.466827565809\n",
    "BsmtFinSF1: \t0.386782893996\n",
    "LotFrontage: \t0.351896380432\n",
    "WoodDeckSF: \t0.32888080889\n",
    "2ndFlrSF: \t0.319192983491\n",
    "OpenPorchSF: \t0.315979580344\n",
    "HalfBath: \t0.284626062633\n",
    "LotArea: \t0.263842911565\n",
    "BsmtFullBath: \t0.227551303799\n",
    "BsmtUnfSF: \t0.214280506904\n",
    "BedroomAbvGr: \t0.168272155796\n",
    "KitchenAbvGr: \t-0.135978700432\n",
    "EnclosedPorch: \t-0.128695108854\n",
    "ScreenPorch: \t0.111378177949\n",
    "PoolArea: \t0.0923894928294\n",
    "MSSubClass: \t-0.0845630196621\n",
    "OverallCond: \t-0.0777543847898\n",
    "MoSold: \t0.046400930621\n",
    "3SsnPorch: \t0.04455301874\n",
    "YrSold: \t-0.0288844993565\n",
    "LowQualFinSF: \t-0.0256424965435\n",
    "MiscVal: \t-0.0212162163774\n",
    "BsmtHalfBath: \t-0.016915050596\n",
    "BsmtFinSF2: \t-0.0109518948727\n",
    "```\n",
    "\n",
    "Here's an examle of the correlation between quality of property & sale price. You can clearly see the linear relationship with a positive slope.\n",
    "\n",
    "![Eval Equation](../assets/ames/quality_price_corr_plot.png)\n",
    "\n",
    "Here are a few more plots of the highest correlated features with target and the linear relationship between them:\n",
    "\n",
    "![Eval Equation](../assets/ames/6_individual_corr_plots.png)\n",
    "\n",
    "Real estate prices are strongly correlated with location (desirable vs un-desirable). Here's a candle stick plot of Neighboorhood to Sale Price:\n",
    "\n",
    "![Eval Equation](../assets/ames/neighborhood_to_saleprice.png)\n",
    "\n",
    "Property type to Sale Price:\n",
    "\n",
    "![Eval Equation](../assets/ames/property_type_to_saleprice.png)\n",
    "\n",
    "From experience we know that the kitchen is an important part of making a decision to purcahse a property for alot of families. The data supports our experience:\n",
    "\n",
    "![Eval Equation](../assets/ames/kitchen_quality_to_saleprice.png)\n",
    "\n",
    "### Skew of Univariate Distributions\n",
    "Skew refers to a distribution that is assumed Gaussian (normal or bell curve) that is shifted or squashed in one direction or another. Many machine learning algorithms assume a Gaussian distribution. Knowing that an attribute has a skew may allow you to perform data preparation to correct the skew and later improve the accuracy of your models. \n",
    "\n",
    "Below is a summary of the skewness in the training dataset provided. \n",
    "\n",
    "Note: Positive (right), zero show less skew and negative (left) skew. Both in & out of time datasets seem to have similar skew properties. I will be applying a log transformation to reduce the impact of skew on the overall model selection & evaluation process.\n",
    "\n",
    "```\n",
    "Id                0.000\n",
    "MSSubClass        1.407\n",
    "LotFrontage       2.163\n",
    "LotArea          12.203\n",
    "OverallQual       0.216\n",
    "OverallCond       0.694\n",
    "YearBuilt        -0.614\n",
    "YearRemodAdd     -0.505\n",
    "MasVnrArea        2.668\n",
    "BsmtFinSF1        1.687\n",
    "BsmtFinSF2        4.265\n",
    "BsmtUnfSF         0.919\n",
    "TotalBsmtSF       1.525\n",
    "1stFlrSF          1.377\n",
    "2ndFlrSF          0.812\n",
    "LowQualFinSF      9.008\n",
    "GrLivArea         1.366\n",
    "BsmtFullBath      0.598\n",
    "BsmtHalfBath      4.102\n",
    "FullBath          0.035\n",
    "HalfBath          0.678\n",
    "BedroomAbvGr      0.212\n",
    "KitchenAbvGr      4.487\n",
    "TotRmsAbvGrd      0.676\n",
    "Fireplaces        0.649\n",
    "GarageYrBlt      -0.650\n",
    "GarageCars       -0.344\n",
    "GarageArea        0.179\n",
    "WoodDeckSF        1.499\n",
    "OpenPorchSF       2.364\n",
    "EnclosedPorch     3.089\n",
    "3SsnPorch        10.301\n",
    "ScreenPorch       4.121\n",
    "PoolArea         14.823\n",
    "MiscVal          24.468\n",
    "MoSold            0.212\n",
    "YrSold            0.097\n",
    "SalePrice         1.882\n",
    "```\n",
    "\n",
    "### Density Distrubtion Plots\n",
    "Density plots are another way of getting a quick idea of the distribution of each attribute. The plots look like an abstracted histogram with a smooth curve drawn through the top of each bin, much like your eye tried to do with the histograms.\n",
    "\n",
    "Notice that some features like YearSold and Fireplaces are not normally distributed... \n",
    "![Eval Equation](../assets/ames/density_distribution_plot.png)\n",
    "\n",
    "![Eval Equation](../assets/ames/density_distribution_plot_2.png)\n",
    "\n",
    "We can take the log of SalePrice to make it normally distributed...\n",
    "\n",
    "![Eval Equation](../assets/ames/saleprice_distribution.png)\n",
    "\n",
    "\n",
    "### Principle Component Analysis\n",
    "Principal Component Analysis (PCA) uses linear algebra to transform the dataset into a compressed form. Generally this is called a data reduction technique. A property of PCA is that you can choose the number of dimensions or principal components in the transformed result. In the example below, we use PCA and select 3 principal components. \n",
    "\n",
    "Looks like most of the variance can be explained within first 3 principe components.\n",
    "![Eval Equation](../assets/ames/cumulative_explained_variance.png)\n",
    "\n",
    "\n",
    "### Algorithms and Techniques\n",
    "This supervized learning project requires I build a regressor. Since I have no idea which algorithms will work well, I will build a test harness aka (see Spot Check below) to evaluate several algorithms based on the Root Meas Squared Error (RMSE) metric. I have implemented a function called `rmse_cv` to perform this evaluation locally using cross validation. This is the metric I will be using and comparing against the Kaggle Public Leaderboard (LB).\n",
    "\n",
    "You can find more details about the test harness in the implementation section. The overall idea is to iterate over a list of regression algorithms listed below and analyze how each performs. I will then take the top 3 performing ones and tune further.\n",
    "\n",
    "Candidate Regression Algorithms (using default parameters documented in scikit-learn docs)\n",
    "\n",
    "+ LASSO \n",
    "  + During EDA, noticed several features were linearly correlated to target. Fitting a straight line through this data may provide to be a winning strategy.\n",
    "  + LASSO is a regression method that involves penalizing the absolute size of the regression coefficients. By penalizing (or equivalently constraining the sum of the absolute values of the estimates) you end up in a situation where some of the parameter estimates may be exactly zero. Automatic feature/variable selection\n",
    "+ CART \n",
    "  + Each root node represents a single input variable (x) and a split point on that variable (assuming the variable is numeric). The leaf nodes of the tree contain an output variable (y) which is used to make a prediction\n",
    "  + A decision tree may be useful in the event where a value for a feature can help split the data by maximizing the information gain. \n",
    "+ KNeighborsRegressor\n",
    "  + K nearest neighbors is an algorithm that stores all available cases and classifies new cases based on a similarity measure (distance functions).\n",
    "  + Since home prices vary by geography, this is why the real estate profession uses comparables to determine sale prices. \n",
    "+ XGBRegressor since 70% of Kaggle competitions are won by XGBoost. \n",
    "  + Decision trees (like CART)\n",
    "  + Similarily for AdaBoostRegressor & GradientBoostingRegressor, boosting algorithms use future weak learners to correct what past weak learners got wrong.  \n",
    "  + Some features are more important to sale price than others. Boosted tree algorithms will help identify a set of feature value splits which will help predict sale price more accurately.\n",
    "+ RandomForestRegressor\n",
    "  + An esemble of weak learners each of which contains a subset of features with bagging might pick up on the structure of the problem to be learned.\n",
    "  + Like evolution, some trees (mutations) will prove to be useful while others will fail. RF will help us run different feature subsets in parallel to observe their predictive power to sale price. \n",
    "+ ExtraTreesRegressor\n",
    "  + Fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and use averaging to improve the predictive accuracy and control over-fitting.\n",
    "  + In the event that only a few features have predictive power, ET will help identify which combination of feature subsets help predict sale price.\n",
    "+ Support Vector Regressor\n",
    "  + An SVM model is a representation of the examples as points in space, mapped so that the examples of the separate categories are divided by a clear gap that is as wide as possible. New examples are then mapped into that same space and predicted to belong to a category based on which side of the gap they fall.\n",
    "  + Since PCA showed that most of the variance could be explained by the first 3 components. Suppor vectors may prove similar value.\n",
    "+ Deep Learning using Keras & TensforFlow\n",
    "  + Say you have two sets of neurons: ones that receive an input signal and ones that send an output signal. When the input layer receives an input it passes on a modified version of the input to the next layer. In a deep network, there are many layers between the input and output, allowing the algorithm to use multiple processing layers, composed of multiple linear and non-linear transformations.\n",
    "  + Since our time for feature engineering is limited and it's more efficient to have an algorithm try all the possible higher dimensional relationship analysis\n",
    "\n",
    "\n",
    "### Benchmark\n",
    "Kaggle maintains a portion of the data for each competition (including this one) as a hold out set which is used to evaluate the generalizability of our model to new unseen data (aka private leaderboard).  \n",
    "\n",
    "Kaggle competitions are decided by a model's performance on a test data set. Kaggle maintains the answers for the test dataset, but withholds them to compare with the submitted predictions. The Public Score is what you receive after each submission (calculated using a statistical evaluation metric described on the Evaluation page). The Public Score is being determined from only a fraction of the test data set (25-33%). This is the Public Leaderboard, and it shows some the performance of your submission relative to others during the competition.\n",
    "\n",
    "When the competition ends, Kaggle takes the selected submissions and score the predictions against the REMAINING FRACTION of the test set. You never receive ongoing feedback about your score on this portion; hence the name (Private leaderboard). Final competition results are based on the Private leaderboard, and the Winner is the person(s) at the top of the Private Leaderboard. This separation of the test set into public and private portions is what ensures that the most accurate but generalized model is the one that wins the challenge. If you based your model solely on the public data which gives you constant feedback, you run the danger of a model that overfits to the specific noise in that data. This addresses one of the hard challenges in data science is to avoid overfitting, by leaving your model flexible to out-of-sample data.\n",
    "\n",
    "My goal for this project was to place among top 20% (RMSE score of 0.12095) of the Kaggle public leaderboard.  \n",
    "\n",
    "In order to perform well, it's critical to define a benchmark and cross validation strategy locally. I created a method to calculate the Root Mean Squared Error and used it locally as I experimented with various algorithms.\n",
    "\n",
    "Since scikit learn has a convention convention to maximize scoring functions, while RSME is minimized. By multiplying by negative one, higher values of negative mean squared error are better than lower values and thus conform to the scikit learn convention.\n",
    "\n",
    "```\n",
    "def rmse_cv(model):\n",
    "    rmse= np.sqrt(-cross_val_score(model, X_train, y, scoring=\"neg_mean_squared_error\", cv = 10))\n",
    "    return(rmse)\n",
    "```\n",
    "\n",
    "## III. Methodology\n",
    "\n",
    "### Data Preprocessing & Feature Engineering\n",
    "For most real world data science projects, data preprocessing often ends up taking a large portion of the effort and time. Fortuantely, this is less of an issue for Kaggle competitions since the data is already represented in a tabular format. \n",
    "\n",
    "Since some learning algorithms are negatively impacted by differing scales of the raw data, I performed a log transformation on the numeric features as well as the target. \n",
    "\n",
    "Additionally, due to the large number of missing values (see EDA above), I used the scikit-learn imputer module to impute the mean for missing values. \n",
    "\n",
    "Both our training & test data sets contain both numerical (quantitative) and categorical (qualitative) features. Since scikit-learn expects a tabular numerical training data set, I must convert all categoricals using one-hot encoding (aka binary features).\n",
    "\n",
    "Features to engineer include but not limited to:\n",
    "\n",
    "+ calculate age of property (relative to oldest)\n",
    "+ years till remodel\n",
    "+ linear combinations of square footage attributes (DID NOT HELP!)\n",
    "+ one hot encoding of categorical features\n",
    "\n",
    "Feature selection will be done via:\n",
    "\n",
    "+ dropping highly correlated features (see correlation section above)\n",
    "+ PCA\n",
    "+ Recusive Feature Elimination (DID NOT HELP!)\n",
    "+ LASSO\n",
    "\n",
    "#### Complications\n",
    "I encountered several complications as the competition got underway.\n",
    "\n",
    "+ I had a million ideas in my head of things I wanted to try, but didn't have luxury of time. As a result, I created a workflow (see above) and stayed disciplined to working through each step of the workflow.\n",
    "+ My first Kaggle submission was bottom of the public leaderboard. I quickly realized there was a formatting error and was pleased with the outcome of the resubmission.\n",
    "+ The more code I developed, the slower it became to execute the entire notebook. In the future, I plan to create multiple notebooks and share artifacts between each to save time.\n",
    " \n",
    "### Implementation\n",
    "\n",
    "#### Evaluate Algorithms (Spot Check) with Standardization\n",
    "\n",
    "I suspect that differing scales of the raw data may be negatively impacting the skill of some of the algorithms. I have performed a standardization in which data is transformed such that each attribute has a mean value of zero and a standard deviation of 1.\n",
    "\n",
    "Please look at the Jupyter notebook for implementation details including parameter values. The parameters used during spot check are the default model parameters provided by scikit-learn. \n",
    "\n",
    "Train:Test split is 50%:50%.\n",
    "\n",
    "According to the figure below, here are the results (lower is better):\n",
    "\n",
    "```\n",
    "Model: RMSE mean (RMSE Std Dev)\n",
    "ScaledLASSO: 0.398762 (0.027284)\n",
    "ScaledRidge: 0.135862 (0.026519)\n",
    "ScaledCART: 0.189709 (0.022052)\n",
    "ScaledKNN: 0.197759 (0.018980)\n",
    "ScaledXGBRegressor: 0.129797 (0.021415) ## 1st Best\n",
    "ScaledGradientBoostingRegressor: 0.125931 (0.018366) ## 2nd Best\n",
    "ScaledAdaBoostRegressor: 0.176386 (0.011455)\n",
    "ScaledRandomForest: 0.150491 (0.016641)\n",
    "ScaledExtraTrees: 0.148834 (0.019462)\n",
    "ScaledSVR: 0.189114 (0.025751)\n",
    "``` \n",
    "\n",
    "![Eval Equation](../assets/ames/scaled_algo_comparison.png)\n",
    "\n",
    "Next, I tuned the hyper-parameters for several of these algorithms above and created a final ensemble model with `.2 * xgboost_predictions + .8 * lasso_predictions`.\n",
    "\n",
    "### Refinement\n",
    "The process of improvement upon the algorithms and techniques is iterative and experiemental in nature. you used in your implementation. I used the `rmse_cv` (Root Mean Squared Error Cross Validation) to help me decide whether a certain change was to be persisted or not. \n",
    "\n",
    "I first started with the best performing model (XGBoost) according to the test harness for model selection mentioned earlier. I then performed feature engineering to create a whole new bunch of signals which I thought would add value including:\n",
    "\n",
    "+ Discretization\n",
    "\t+ Tranform Neighborhoods into {Poor, MiddleIncome, Affluent} based on the SalePrice in training data  \n",
    "+ Linear construction\n",
    "\t+ create new features to combine numeric features with various operators (x_i + x_j, x_i * x_j, x_i ^ 2, log x_i )\n",
    "\n",
    "These features didn't help my local cross validation score for XGBoost but did help for LASSO.\n",
    "\n",
    "I've documented the series of refinements I've made along with Kaggle submissions for promising refinements.\n",
    "\n",
    "**Refinement Workflow Results**\n",
    "\n",
    "| Model Description       | Local CV        |  LeaderBoard   |  \n",
    "| ------------- |:-------------:| -----:|\n",
    "| baseline: forgot to tranform predicted results using exponential     | \t0.132 | Bottom 90% |\n",
    "| baseline: np.expm1(target)    | \t\t0.132 | 0.1335 | \n",
    "| Dropped 10 least important features (according to XGBoost feature importances)   | \t\t0.145 | - | \n",
    "| Feature construction using strategies mentioned above   | \t\t0.132 | - | \n",
    "| XGBoost Hyper-parameter tuning colsample_bylevel    | \t\t0.128 | 0.1354 | \n",
    "| LASSO   |  0.1226 | - | \n",
    "| LASSO w/ YearBuilt & YearRemodAdd engineered features  |  0.12243 | - | \n",
    "| Baseline Keras using TensorFlow |  0.1602 | - | \n",
    "| Keras using TensorFlow Deeper Network Topology  |  0.1602  | - | \n",
    "| Keras using TensorFlow Wider Network Topology  |  0.1602  | - | \n",
    "| Ensemble XGBoost (60%) with ExtraTrees (40%)   | \t\t0.131 | 0.1364 | \n",
    "| Ensemble XGBoost (30%) with LASSO (70%)    |  0.128 | 0.1202 (top 17%)| \n",
    "| Ensemble XGBoost (20%) with LASSO (80%)    |  0.128 | 0.1203 (top 15%)| \n",
    "| Ensemble XGBoost (20%) with LASSO (80%) w/ YearBuilt & YearRemodAdd engineered features  |  0.12243 | 0.11968 (top 14%) |\n",
    "| Ensemble XGBoost (20%) with LASSO (80%) w/ linear combination of engineered features  |  0.12255 | - |  \n",
    "| Ensemble XGBoost (20%) with LASSO (80%) w/ highly correlated features dropped |  0.123962 | 0.11965 | \n",
    "\n",
    "\n",
    "## IV. Results\n",
    "\n",
    "### Model Evaluation and Validation\n",
    "\n",
    "The iterative refinement workflow documented above led me to the final model which is an esemble of two submodels (XGBoost & LASSO). I've validated the model's performance with the public leaderboard and am currently among top 14%. My baseline score to beat was 0.12095 and my current score is 0.11965, improvement of 0.0013. \n",
    "\n",
    "#### Final Model Parameters:\n",
    "\n",
    "```\n",
    "LassoCV(alphas=0.0005, copy_X=True, cv=None, eps=0.001,\n",
    "    fit_intercept=True, max_iter=1000, n_alphas=100, n_jobs=1,\n",
    "    normalize=False, positive=False, precompute='auto', random_state=None,\n",
    "    selection='cyclic', tol=0.0001, verbose=False)\n",
    "```    \n",
    "\n",
    "Note: Lasso picked 109 variables and eliminated the other 175 variables\n",
    "\n",
    "\n",
    "```\n",
    "XGBRegressor(base_score=0.5, colsample_bylevel=1, colsample_bytree=1, gamma=0,\n",
    "       learning_rate=0.1, max_delta_step=0, max_depth=4,\n",
    "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
    "       objective='reg:linear', reg_alpha=0, reg_lambda=1,\n",
    "       scale_pos_weight=1, seed=0, silent=True, subsample=1)\n",
    "```\n",
    "\n",
    "One of the crucial aspects for my model to work is to perform the same transformations on training vs production (hold out) test set. I've tested the model against the test set provided by Kaggle. I've made sure this is done in a consistent manner which prevents leakage from occuring as well as well as being robust to small perturbations (log feature transformations). The generalizability of the model will be evident at the end of competition. The results can be trusted since it takes care of the precautions mentioned above.\n",
    "\n",
    "### Justification\n",
    "As you can see from the refinement result table, the final results found are stronger than the benchmark result reported earlier. The final model is a linear combination (80%) LASSO and (20%) XGBoost. Due to the high ranking on the public leaderboard, I conclude the model has sufficient predictive power to predict Sale Price.\n",
    "\n",
    "Furthermore, the output of the feature importance tables provided additional support as it's consistent with our domain knowledge and our intution. \n",
    "\n",
    "## V. Conclusion\n",
    "\n",
    "### Free-Form Visualization\n",
    "\n",
    "### RSME (Train vs Test)\n",
    "\n",
    "You can observe that the RSME for training is much lower than test which expected since we trained using the training data set and our test set doesn't contain targets. You can also see that the model doesn't perform any better (flat line) after 50 estimators on the test set.\n",
    "\n",
    "![Eval Equation](../assets/ames/train_vs_test_rsme_xgboost.png)\n",
    "\n",
    "### Feature Importances\n",
    "Model interpretability is key to build trust in the output of the model. Feature importance table provides the the contributions of each feature to the final model in sorted order.\n",
    " \n",
    "#### XGBoost Feature Importances\n",
    "\n",
    "![Eval Equation](../assets/ames/xgboost_feature_importances.png)\n",
    "\n",
    "#### LASSO Feature Importances\n",
    "\n",
    "```\n",
    "0.43238483795 HouseStyle\n",
    "0.306658079877 LotArea\n",
    "0.232434529655 Street\n",
    "0.123212466816 MasVnrArea\n",
    "0.0971238714301 SaleCondition\n",
    "0.0808617139641 ScreenPorch\n",
    "0.0661590211202 LotFrontage\n",
    "0.049453316714 YearBuilt\n",
    "0.0334307485067 OverallQual\n",
    "0.0325227384018 Exterior2nd\n",
    "0.0319212149676 YearRemodAdd\n",
    "0.0281572640839 FireplaceQu\n",
    "0.023089079368 Neighborhood\n",
    "0.0226932484255 Condition1\n",
    "0.0137705292927 LowQualFinSF\n",
    "0.0118515554231 EnclosedPorch\n",
    "0.0106919987952 OpenPorchSF\n",
    "0.00847464379466 Utilities\n",
    "0.00831218227088 BsmtFullBath\n",
    "0.008269826877 BsmtExposure\n",
    "0.00729119609035 Fireplaces\n",
    "0.00547443378409 GarageArea\n",
    "0.00483136123582 ExterCond\n",
    "0.00370262728343 BsmtFinType1\n",
    "0.00286553088743 BedroomAbvGr\n",
    "0.00276529252337 TotalBsmtSF\n",
    "0.00268664585463 BsmtCond\n",
    "0.00232346460797 Foundation\n",
    "9.49548227015e-05 LandContour\n",
    "0.0 YrSold\n",
    "0.0 WoodDeckSF\n",
    "0.0 TotRmsAbvGrd\n",
    "-0.0 RoofStyle\n",
    "-0.0 PoolQC\n",
    "0.0 PavedDrive\n",
    "0.0 OverallCond\n",
    "0.0 MasVnrType\n",
    "0.0 MSZoning\n",
    "0.0 LotShape\n",
    "-0.0 KitchenQual\n",
    "0.0 KitchenAbvGr\n",
    "0.0 HeatingQC\n",
    "0.0 Heating\n",
    "-0.0 GarageYrBlt\n",
    "-0.0 GarageQual\n",
    "-0.0 GarageFinish\n",
    "0.0 GarageCond\n",
    "-0.0 GarageCars\n",
    "-0.0 Functional\n",
    "0.0 FullBath\n",
    "0.0 Exterior1st\n",
    "-0.0 ExterQual\n",
    "-0.0 Electrical\n",
    "-0.0 BsmtQual\n",
    "-0.0 BsmtHalfBath\n",
    "0.0 BsmtFinType2\n",
    "-0.0 BsmtFinSF2\n",
    "0.0 Alley\n",
    "0.0 3SsnPorch\n",
    "0.0 1stFlrSF\n",
    "-0.000413127896622 GrLivArea\n",
    "-0.00082409880122 2ndFlrSF\n",
    "-0.00142257874049 LotConfig\n",
    "-0.0029507208516 BsmtFinSF1\n",
    "-0.00498385472283 LandSlope\n",
    "-0.00569681578958 Condition2\n",
    "-0.00683008097597 BldgType\n",
    "-0.00715378950237 SaleType\n",
    "-0.00869858413963 MiscFeature\n",
    "-0.0136377572739 MSSubClass\n",
    "-0.0158043933627 MiscVal\n",
    "-0.0172049027349 MoSold\n",
    "-0.0175369827903 GarageType\n",
    "-0.0236894322544 RoofMatl\n",
    "-0.0325322135113 Fence\n",
    "-0.0333092460561 CentralAir\n",
    "-0.0397138716501 HalfBath\n",
    "-0.0670340111069 PoolArea\n",
    "-0.292551006308 BsmtUnfSF\n",
    "```\n",
    "\n",
    "### Hyperparameter vs Cross Validation Score\n",
    "This plot illustrates the RMSE relative to various values for `alpha`. You can see that the minimum RMSE is achieved when `alpha=10` and greater values of alpha results in larger (undesirable) values for RMSE.\n",
    "\n",
    "![Eval Equation](../assets/ames/xgboost_validation_alpha_tune.png)\n",
    "\n",
    "### SalePrice Outlier Analysis\n",
    "It's important to spot outliers in our target. The IQR range seems to be from 120K - 205K.\n",
    "![Eval Equation](../assets/ames/saleprice_whisker.png )\n",
    "\n",
    "### Target Variable Outlier Analysis\n",
    "There were a number of values in the target variable which were outliers. It's important to note the IQR range and to spot check them against our predicted results.\n",
    "\n",
    "![Eval Equation](../assets/ames/saleprice_whisker.png )\n",
    "\n",
    "### Reflection\n",
    "This project was a great opportunity to define & implement my end-to-end data science workflow using a results oriented approach.\n",
    "\n",
    "The workflow is as follows:  \n",
    "\n",
    "1. Problem Definition (Ames house price data). \n",
    "\t+ Experimental Design: identify data sources, formats, data dictionary, features and target. \n",
    "\n",
    "2. Loading the Dataset\n",
    "\t+ Load and pre-process the data into a representation which is ready for model training\n",
    "\n",
    "3. Exploratory Data Analysis \n",
    "\t+ Gather insights by using exploratory methods (univariate feature distributions, skew and correlated attributes to name a few)\n",
    "\n",
    "4. Feature Engineering  \n",
    "  + calculate age of property (relative to oldest)\n",
    "  + years till remodel\n",
    "  + linear combinations of square footage attributes (DID NOT HELP!)\n",
    "  + one hot encoding of categorical features\n",
    "  \n",
    "5. Feature Selection\n",
    "  + dropping highly correlated features (see correlation section above)\n",
    "  + PCA\n",
    "  + Recusive Feature Elimination (DID NOT HELP!)\n",
    "  + LASSO\n",
    "\n",
    "6. Evaluate Algorithms\n",
    "\n",
    "7. Evaluate Algorithms with Standardization\n",
    "\n",
    "8. Algorithm Tuning\n",
    "\t+ Use GridSearch to search & tune hyper-parameters\n",
    "\n",
    "9. Ensemble Methods (such as Bagging and Boosting, Gradient Boosting looked good).  \n",
    "\n",
    "10. Finalize Model (use all training data and confirm using validation dataset).\n",
    "\n",
    "**Interestings**\n",
    "\n",
    "Having gone through the iterative process of model selection & evaluation with the public leaderboard, I'm confident I'll be able to do this in the future for more comprehensive projects.\n",
    "\n",
    "I was also quite pleased with the support Kaggle users provide via the forums. Taking the time to go through the questions and kernels is a great learning resource for practitioners. I learned some new techniques which I'm looking forward to using in the future such as identifying how the private and public data sets were split as this information will help me build a more generalizable model.\n",
    "\n",
    "**Challenges**\n",
    "One of the challenges with working with Kaggle is to make sure you're not overfitting to the public leaderboard. It's tempting to see your score rise and sometimes there are quick shortcuts which may raise your score but may not necessarily make sense. It's important to keep a disciplined approach during model selection and evaluation. \n",
    "\n",
    "It's also critical to structure your code in a way which produces repeatable & reproducible results. Writing modular code which is easy to communicate with others,  \n",
    "\n",
    "### Improvement\n",
    "I plan to work on this Kaggle competition well after my capstone project submission. I believe that engineering new features from existing ones will be the key to building a generalizable model. For example, experimenting with newly created features which are linear combinations of existing ones (BsmtUnfSF/TotalBsmtSF) may provide some lift in the score.\n",
    "\n",
    "One of the activities I found tremendously useful is reading through the forums. Not only do you find code refactoring opportunitties, but you also gain insight into how others decide to approach this learning problem. Data science often involves experimentation and contributing my EDA as well as baseline model is an improvement both personally and to the community at large.\n",
    "\n",
    "Another improvement I could do is define a more aggresive strategy for dealing with outliers. The most obvious is to remove them.\n",
    "\n",
    "-----------"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py2k]",
   "language": "python",
   "name": "conda-env-py2k-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
